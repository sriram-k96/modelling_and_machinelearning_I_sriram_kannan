---
title: "Homework - I"
author: ' Sriram Kannan'
output:
  github_document
---



```{r}
library('class')
library('dplyr')
load(url('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda'))
dat <- ESL.mixture
```


```{r}
plot_mix_data <- expression({
  plot(dat$x[,1], dat$x[,2],
       col=ifelse(dat$y==0, 'blue', 'orange'),
       pch=20,
       xlab=expression(x[1]),
       ylab=expression(x[2]))
  ## draw Bayes (True) classification boundary
  prob <- matrix(dat$prob, length(dat$px1), length(dat$px2))
  cont <- contourLines(dat$px1, dat$px2, prob, levels=0.5)
  rslt <- sapply(cont, lines, col='purple')
})

eval(plot_mix_data)
```


1) Re-write the functions fit_lc and predict_lc using lm, and the associated predict method for lm objects.

```{r}
#lm classifier
data <- data.frame(y = dat$y, x1 = dat$x[,1], x2 = dat$x[,2])
data
fit_lm <- lm(y ~ x1+x2, data = data)
summary(fit_lm)
predict_lm <- predict(fit_lm, newdata = data.frame(dat$xnew))



```



```{r}
## reshape predictions as a matrix
pred_lm <- matrix(predict_lm, length(dat$px1), length(dat$px2))
contour(pred_lm,
      xlab=expression(x[1]),
      ylab=expression(x[2]))
```


```{r}
## find the contours in 2D space such that lc_pred == 0.5
cont_lm <- contourLines(dat$px1, dat$px2, pred_lm, levels=0.5)

## plot data and decision surface
eval(plot_mix_data)
sapply(cont_lm, lines)
```

Consider making the linear classifier more flexible, by adding squared terms for x1 and x2 to the linear model

```{r}
fit_lm_squared <- lm(y ~ poly(x1,2)+poly(x2,2), data = data)
predict_lm_squared <- predict(fit_lm_squared, newdata = data.frame(dat$xnew))
pred_lm_squared <- matrix(predict_lm_squared, length(dat$px1), length(dat$px2))
contour(pred_lm,
      xlab=expression(x[1]),
      ylab=expression(x[2]))

cont_lm_squared <- contourLines(dat$px1, dat$px2, pred_lm_squared, levels=0.5)

## plot data and decision surface
eval(plot_mix_data)
sapply(cont_lm_squared, lines)

```
Describe how this more flexible model affects the bias-variance tradeoff


Bias: 

An error due to Bias is the distance between the predictions of a model and the true values. In models with high bias, the model pays little attention to training data and oversimplifies the model and learns the wrong relations by not taking in account all the features adequately.

Variance: 

Variance error is the Variability of model prediction for given data(The model is usually very complex). In models with high variance, the model pays a lot of attention to training data, to the point of memorizing the data rather than learning from it. A model with a high variance is not flexible to generalize to unseen data (test data).

Bias - Variance Tradeoff : 

Bias-variance trade-off is tension between the error introduced by the bias and the error produced by variance. Bias- Variance trade-off is about balancing and finding a sweet spot between the two types of errors.

In case of the linear model using mixture data, we see that our model has a high bias due to a large number of orange points and blue points being predicted wrongly. The same is true in the case of the more flexible model with squared terms. There seems to be a slight curve and hence an increase in variance compared to the original linear model but the bias is still high enough that it's safe to conclude that the model is still predominantly biased. Models which either have a high bias or a high variance tend to perform poorly with test data. 
